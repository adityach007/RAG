{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S57LMKTuWFs",
        "outputId": "ed76b51d-bcca-4136-80a4-ef8987561f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m425.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"your_groq_api_key\""
      ],
      "metadata": {
        "id": "Px1CCOwnuZqF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generator**"
      ],
      "metadata": {
        "id": "-zU6tNn4vC-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key = groq_api_key)\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "  text_input = '\\n'.join(itext)\n",
        "  prompt = f\"Please elaborate on the folleoing content: \\n{text_input}\"\n",
        "\n",
        "  try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature = 0.1\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "  except Exception as e:\n",
        "    return str(e)"
      ],
      "metadata": {
        "id": "AcTOJfEsuhi9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# This function is used to wrap 80 characters per line\n",
        "\n",
        "def print_formatted_response(response):\n",
        "  wrapper = textwrap.TextWrapper(width=80)\n",
        "  text_wrapped = wrapper.fill(text=response)\n",
        "\n",
        "  print(\"Response -------------------\")\n",
        "  print(response)\n",
        "  print(\"Formatted response -------------------\")\n",
        "  print(text_wrapped)"
      ],
      "metadata": {
        "id": "mm99-ZuOwu3L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data**"
      ],
      "metadata": {
        "id": "R_jDXOWQxruz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ],
      "metadata": {
        "id": "8AMms-BaxaFr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "paragraph = ' '.join(db_records)          # Joins the db_records text into a single line string\n",
        "wrapper = textwrap.TextWrapper(width=80)  # Creates wrapper of 80 characters per line\n",
        "text_wrapped = wrapper.fill(text = paragraph)\n",
        "\n",
        "print(text_wrapped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR__9zShxstZ",
        "outputId": "dd294e3b-ffb1-4afb-ae0e-94db057d79c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Query**"
      ],
      "metadata": {
        "id": "unFSPWpRyuSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"define a rag store\""
      ],
      "metadata": {
        "id": "bEV6jbGHyDJ6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be performing generation without doing any augmentation"
      ],
      "metadata": {
        "id": "EUSn6hx4y8H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otbk4ogcyvvQ",
        "outputId": "6fe76863-47d4-406a-90c2-e68a8de1bd96"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response -------------------\n",
            "It appears that the given content is a jumbled collection of letters, possibly forming a phrase or sentence when rearranged. Let's break it down and try to make sense of it.\n",
            "\n",
            "The letters are:\n",
            "d, e, f, i, n, e, a, r, a, g, s, t, o, r, e\n",
            "\n",
            "Upon closer inspection, it seems that these letters can be rearranged to form a coherent phrase: \"Define a storage\".\n",
            "\n",
            "Here's a possible rearrangement:\n",
            "\n",
            "* \"Define\" is a verb that means to explain or describe something.\n",
            "* \"A\" is an indefinite article.\n",
            "* \"Storage\" refers to a place or system for storing things.\n",
            "\n",
            "So, the rearranged phrase \"Define a storage\" could be a prompt or instruction to explain or describe a storage system or facility.\n",
            "\n",
            "Is there anything specific you'd like to know or discuss about this phrase, or would you like me to help with anything else?\n",
            "Formatted response -------------------\n",
            "It appears that the given content is a jumbled collection of letters, possibly\n",
            "forming a phrase or sentence when rearranged. Let's break it down and try to\n",
            "make sense of it.  The letters are: d, e, f, i, n, e, a, r, a, g, s, t, o, r, e\n",
            "Upon closer inspection, it seems that these letters can be rearranged to form a\n",
            "coherent phrase: \"Define a storage\".  Here's a possible rearrangement:  *\n",
            "\"Define\" is a verb that means to explain or describe something. * \"A\" is an\n",
            "indefinite article. * \"Storage\" refers to a place or system for storing things.\n",
            "So, the rearranged phrase \"Define a storage\" could be a prompt or instruction to\n",
            "explain or describe a storage system or facility.  Is there anything specific\n",
            "you'd like to know or discuss about this phrase, or would you like me to help\n",
            "with anything else?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Advanced Techniques and Evaluation**"
      ],
      "metadata": {
        "id": "k20V_3QY0CyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Retrieval metrics**"
      ],
      "metadata": {
        "id": "4OAJVbPR0Qvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cosine Similarity**"
      ],
      "metadata": {
        "id": "qpx7NG-W3g36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Why TF-IDF?\n",
        "#Term Frequency-Inverse Document Frequency (TF-IDF) quantifies the relevance of a word\n",
        "#to a document in a collection, distinguishing common words from those significant to specific texts.\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "  vectorizer = TfidfVectorizer(\n",
        "      stop_words = 'english',\n",
        "      use_idf = True,\n",
        "      norm = 'l2',            # L2 normalization\n",
        "      ngram_range = (1, 2),     # uasge of unigrams and bigrams\n",
        "      sublinear_tf = True,\n",
        "      analyzer = 'word'  # tells what is the level of the TF_IDF and can be done on character level also\n",
        "  )\n",
        "\n",
        "  tf_idf = vectorizer.fit_transform([text1, text2])\n",
        "  # used fit for leanring the vocabulary and weights from both the query and text\n",
        "  # and used transform to convert words into vectors\n",
        "\n",
        "  similarity = cosine_similarity(tf_idf[0], tf_idf[1])\n",
        "  return similarity[0][0]         # returned in this way because cosine similarity returns a single value matrix"
      ],
      "metadata": {
        "id": "uhoZeaID0CpT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity has disadvantage as it just calcualtes the maths i.e. the distance between two text as vectors which might sometimes leaves the contextual and semantic understanding between the query asked and the corpus."
      ],
      "metadata": {
        "id": "88oZRyM23sCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enhanced Similarity**"
      ],
      "metadata": {
        "id": "TPdGNBhs3kD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method preprocesses texts to reduce noise, expands terms with synonyms from WordNet, and computes similarity based on the semantic richness of the expanded vocabulary. This method aims to improve the accuracy of similarity assessments between two texts by considering a broader context than typical direct comparison methods."
      ],
      "metadata": {
        "id": "BKi8Vd9a4Nql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_synonyms(word):\n",
        "\n",
        "  synonyms = set()\n",
        "  for synonym in wordnet.synsets(word):     # captures the synonyms or different senses of the particular word\n",
        "    for lemma in synonym.lemmas():           # converts the senses in variations of words\n",
        "      synonyms.add(lemma.name())\n",
        "\n",
        "  return synonyms\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "  doc = nlp(text.lower())\n",
        "  lemmatized_words = []\n",
        "\n",
        "  for token in doc:\n",
        "    if token.is_stop or token.is_punct:\n",
        "      continue\n",
        "    lemmatized_words.append(token.lemma_)\n",
        "\n",
        "  return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "  expanded_words = words.copy()\n",
        "\n",
        "  for word in words:\n",
        "    expanded_words.extend(get_synonyms(word))\n",
        "\n",
        "  return expanded_words\n",
        "\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "    # Preprocess and tokenize texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Expand with synonyms\n",
        "    words1_expanded = expand_with_synonyms(words1)\n",
        "    words2_expanded = expand_with_synonyms(words2)\n",
        "\n",
        "    # Count word frequencies\n",
        "    freq1 = Counter(words1_expanded)\n",
        "    freq2 = Counter(words2_expanded)\n",
        "\n",
        "    # Create a set of all unique words\n",
        "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vector1 = [freq1[word] for word in unique_words]\n",
        "    vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "    return cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boihAn690Clp",
        "outputId": "46784343-e48e-4def-9d79-e4ae703daedc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive RAG**"
      ],
      "metadata": {
        "id": "R430reamDHfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Keyword search and matching**"
      ],
      "metadata": {
        "id": "0X7jQiUNDJPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "  best_score = 0\n",
        "  best_record = None\n",
        "\n",
        "  # Now split the query for individual keywords\n",
        "  query_keywords = set(query.lower().split())\n",
        "\n",
        "  for record in db_records:\n",
        "    record_keywords = set(record.lower().split())\n",
        "    score = query_keywords.intersection(record_keywords)\n",
        "    current_score = len(score)\n",
        "\n",
        "    if current_score > best_score:\n",
        "      best_score = current_score\n",
        "      best_record = record\n",
        "\n",
        "  return best_score, best_record"
      ],
      "metadata": {
        "id": "sAEbrsMZ0Cg1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "print(f\"Best keyword score: {best_keyword_score}\")\n",
        "print(f\"Best matching record: {best_matching_record}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CYvX3gB0CeI",
        "outputId": "2ebf8835-7845-433c-e5d6-c9de46188220"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best keyword score: 3\n",
            "Best matching record: A RAG vector store is a database or dataset that contains vectorized data points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Metrics**"
      ],
      "metadata": {
        "id": "1DPhL0QPFP6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity\n",
        "\n",
        "score = calculate_cosine_similarity(query, best_matching_record)\n",
        "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUffgdGg0CbS",
        "outputId": "62485111-a7e7-4721-8a1b-9fba57f76606"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Similarity\n",
        "\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBs2OmorIZOi",
        "outputId": "b9a5a4b6-4408-40da-b482-a149e352fb31"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Augmented input**"
      ],
      "metadata": {
        "id": "aOCdxzWSJfTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input=query+ \": \"+ best_matching_record\n",
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQHmZaMS0CYy",
        "outputId": "24063a90-44d0-4501-ae09-df8de5b1a6f2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response -------------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Formatted response -------------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generation**"
      ],
      "metadata": {
        "id": "7_4BJ-YJJqZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7F_I4YUzO2H",
        "outputId": "e9278aa4-25ca-4334-d0f3-960834da7bf2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response -------------------\n",
            "The given content appears to be a definition of a specific data structure in the field of computer science, particularly in the context of Natural Language Processing (NLP) and machine learning. Let's break it down:\n",
            "\n",
            "**Define a RAG store:**\n",
            "\n",
            "A RAG (Relational Augmented Graph) store is a type of database or data repository that contains vectorized data points. \n",
            "\n",
            "**Key components:**\n",
            "\n",
            "1. **Vectorized data points**: In this context, vectorized data points refer to numerical representations of data, where each data point is represented as a vector in a high-dimensional space. This allows for efficient storage, retrieval, and manipulation of the data.\n",
            "2. **RAG store**: The RAG store is a database or data structure that contains these vectorized data points. It is designed to support efficient querying, retrieval, and manipulation of the data.\n",
            "\n",
            "**Characteristics:**\n",
            "\n",
            "The RAG store has the following characteristics:\n",
            "\n",
            "1. **Contains vectorized data points**: The RAG store contains data points that have been converted into numerical vectors, which enables efficient processing and analysis.\n",
            "2. **Supports querying and retrieval**: The RAG store allows for efficient querying and retrieval of the data points, which is essential for various applications in NLP and machine learning.\n",
            "3. **Designed for relational data**: The RAG store is designed to handle relational data, which means it can store and manage data that has relationships between different entities.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "RAG stores have various applications in NLP and machine learning, including:\n",
            "\n",
            "1. **Information retrieval**: RAG stores can be used to build efficient information retrieval systems that can quickly retrieve relevant data points based on a query.\n",
            "2. **Question answering**: RAG stores can be used to build question answering systems that can efficiently retrieve and rank relevant data points to answer a given question.\n",
            "3. **Text classification**: RAG stores can be used to build text classification systems that can efficiently classify text into different categories based on the vectorized data points.\n",
            "\n",
            "In summary, a RAG store is a data structure that contains vectorized data points and is designed to support efficient querying, retrieval, and manipulation of the data. It has various applications in NLP and machine learning, including information retrieval, question answering, and text classification.\n",
            "Formatted response -------------------\n",
            "The given content appears to be a definition of a specific data structure in the\n",
            "field of computer science, particularly in the context of Natural Language\n",
            "Processing (NLP) and machine learning. Let's break it down:  **Define a RAG\n",
            "store:**  A RAG (Relational Augmented Graph) store is a type of database or data\n",
            "repository that contains vectorized data points.   **Key components:**  1.\n",
            "**Vectorized data points**: In this context, vectorized data points refer to\n",
            "numerical representations of data, where each data point is represented as a\n",
            "vector in a high-dimensional space. This allows for efficient storage,\n",
            "retrieval, and manipulation of the data. 2. **RAG store**: The RAG store is a\n",
            "database or data structure that contains these vectorized data points. It is\n",
            "designed to support efficient querying, retrieval, and manipulation of the data.\n",
            "**Characteristics:**  The RAG store has the following characteristics:  1.\n",
            "**Contains vectorized data points**: The RAG store contains data points that\n",
            "have been converted into numerical vectors, which enables efficient processing\n",
            "and analysis. 2. **Supports querying and retrieval**: The RAG store allows for\n",
            "efficient querying and retrieval of the data points, which is essential for\n",
            "various applications in NLP and machine learning. 3. **Designed for relational\n",
            "data**: The RAG store is designed to handle relational data, which means it can\n",
            "store and manage data that has relationships between different entities.\n",
            "**Applications:**  RAG stores have various applications in NLP and machine\n",
            "learning, including:  1. **Information retrieval**: RAG stores can be used to\n",
            "build efficient information retrieval systems that can quickly retrieve relevant\n",
            "data points based on a query. 2. **Question answering**: RAG stores can be used\n",
            "to build question answering systems that can efficiently retrieve and rank\n",
            "relevant data points to answer a given question. 3. **Text classification**: RAG\n",
            "stores can be used to build text classification systems that can efficiently\n",
            "classify text into different categories based on the vectorized data points.  In\n",
            "summary, a RAG store is a data structure that contains vectorized data points\n",
            "and is designed to support efficient querying, retrieval, and manipulation of\n",
            "the data. It has various applications in NLP and machine learning, including\n",
            "information retrieval, question answering, and text classification.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DBayNB5Jr8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}